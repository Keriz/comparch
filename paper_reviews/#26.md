==+== Computer Architecture Paper Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" UNLESS DIRECTED!
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://safari.ethz.ch/review/architecture21/offline

==+== =====================================================================
==+== Begin Review
==+== Reviewer: Guillaume Thivolet <gthivolet@student.ethz.ch>

==+== Paper Number

26

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

()

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

5

==+== B. Reviewer expertise
==-== Choices:
==-==    1. No familiarity
==-==    2. Some familiarity
==-==    3. Knowledgeable
==-==    4. Expert
==-== Enter the number of your choice:

3

==+== C. Paper summary

(written in 1968)

The upcoming bottleneck for computer engineering resides in where the data is computed. As the processor's gates scale down and speed will increase, the traditional von Neumann architecture will have a congestion node between the data holding resources (memory) and the computing unit.

The author presents a design in which main memories act as extended cache to which the main processing unit can relay instructions for in-memory computing. This would hide the latency to access slower memories, enabling high speed buffers. He proposes several new instructions that could be issued to remote caches (search on mask equality, masked threshold
copy tag bit, sector ADD/Scale..) to take advantage of the data-locality for certain types of operations (ie. matrix multiplications).

Going forward, we need to improve the programs to instruct them on where to compute the data: distinguishing applications that require high-speed memories and those not affected by conventional slower ones. Lessons need to be learned on how to approach an efficient foreseeing of data movement to not hinder performances.

==+== D. Comments for author



==+== E. Comments for PC

==-== Hidden from authors.

The essay is well structured and shows that computer scientists were aware of this memory computing possibility decades ago.
There is a strong emphasis on the challenges such a system would imply.

It is important that as few as possible control commands are sent ahead to help the cache to organize its data, which would otherwise reduce the benefit of this logic-in-memory.

An understated point is the inefficiency of floating-point computation and how optimizing data-storage in relation to the types could fasten the computations.

I would propose an array of memories where each unit is specialized in a certain type of data (2s complement, floating-point, fixed-point), but with a varying mantissa-exponent / fractional size. Large-scale bulk operations would hence be beneficial for much more applications. This would on the other hand complexify the control logic to decide which memory to use, and also require additional delay/conversion loss when transferring data somewhere else. However the in-memory logic could benefit of streamlined, precise arithmetics even more at the cost of making compiler's and programmer's job much harder. Ultimately this depends on the technology used to store the data, the added overhead and how fast the new system becomes. Which means that there's still a lot of work ahead of us!

The conclusion summarizes very well the tight-coupling between programs and the architecture they run on: keep it as simple as possible. As much as we can improve each block, we as programmers are piloting its actions and those need to be easy to comprehend in order to reach maximum performances of any system.

# Strength

- Idea that "remote" cache can be invisible to program
- Ballpark figures to appreciate the gains
- Visionary essay

# Weaknesses

- Requires an independent mechanism to transfer data in the cache as necessary (today's prefetching)
- On what should we concentrate efforts to achieve logic-in-memory? Vague conclusion

==+== Scratchpad (for unsaved private notes)

The goal is this paper is to demonstrate that the future goal will be to have main memories act as caches.
Logic In Memry arays ould be associative 
could be used for error-code correcting aimplementation and sorting
how a class of logic in memory arrays can be used as high soeed buffer memories 
mempory management tecghniques well proven in practive that hide the latency to access the slower, magnetic drum or disc based memory insyead of the microelectronic technologies

Describes the cache/main memory interfaacer
argues that slwoer memories are still OK because of the cache mechanisms that can be implemented
move instructions issued to the array of caches which are supposed to withhold all the data needed to perform the operations


Proposes new operations with a sector memory address, memory address, operands 

Search on mask equality, masked thresold ( > or >=)
Copy tag bit
Sector ADD/Scale 

Actual utility of aforementioned operations is to be seen, as it is effectively hard for a programmer and a compiler to translate the assembly code into the correct operations to take as much profit as possible of these logic-in-memory.
Shift of thoughts: instructions are sent to control the cache, but rather to manipulaet the data in the memory.

--- mechanism that is indepent is transfering the data uin the cache as necessary ( today's prefetching)


++ cache is invisible to the programs (no control commands sent ahead to help the cache to organize its data)

Mentions the utility for matrix multiplications (ex Gauss-Joradan erduction for matrix inversion)



//// New access mode in the memory
Bit(s) slice of data to be accessed (from multiple words at a time) -? a very large number of words can be accessed simulnateously to peform logical operations, by sector on the data in the cache (mxn opertations SIMUL)0)

Sumarry and conclusion

Diverging from the von Neumann organization, provided that the cost of cache processing remains low
High level languages give a subset of instructions to the programmer and less access to the machine being programmed
parallel development of languages and computers


==+== End Review