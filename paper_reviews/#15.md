==+== Computer Architecture Paper Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" UNLESS DIRECTED!
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://safari.ethz.ch/review/architecture21/offline

==+== =====================================================================
==+== Begin Review
==+== Reviewer: Guillaume Thivolet <gthivolet@student.ethz.ch>

==+== Paper Number

15

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

Ready

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

4

==+== B. Reviewer expertise
==-== Choices:
==-==    1. No familiarity
==-==    2. Some familiarity
==-==    3. Knowledgeable
==-==    4. Expert
==-== Enter the number of your choice:

1

==+== C. Paper summary

Graph-processing workloads are increasing due to big-data. Due to the nature of the algorithms that require neighbor traversal, the access patterns are random and have poor memory locality. Tesseract introduces a Processing-In-Memory (PIM) accelerator to efficiently serve the memory requests while fully utilizing the memory bandwidth to its advantage.

Tesseract accelerates graph-processing through 3 major advancements: specialiazed hardware, a dedicated programming interface, and tailored prefetching. It is architected around vaults that each contain an in-order core, several DRAM banks and a memory controller. A crossbar network interconnects each vault in a 3D-stacked DRAM, commonly called a Hybrid Memory Cube (HMC).

One major bottleneck to achieve higher memory bandwidth is the efficient communication between Tesseract cores. To this end, an API introduces a set of functions primitives to allow for vaults to update/request memory information, carry out core synchronization, and send message-prefetching hints.

The prefetcher mechanism consists of message-triggered prefetching, from the previously introduced API, and of a list prefetcher. The former prevents execution of a non-blocking function call before the requested the memory address is prefetched, which makes exact prefetching without any stalls from the core standpoint. The latter is a stride prefetcher using a reference prediction table (RPT). The prefetching data (start, size, stride) are also passed by the API between vaults and is saved in a multiple-entries list utilized in the for-loops for the RPT (for instance when traversing vertices).

Its evaluation has been carried out with five memory intensive graph algorithms. Performance benchmarking shows that Tesseract implemented on DDR3-OoO, HMC(OoO and in-order) and without prefetching beats other conventional systems, still limited by their off-chip link bandwidth (102GB/s VS 8TB/s for DDR3 and HMC, respectively). Furthermore, the programming model doubles the performance increase and makes Tesseract relevant even when normal architectures hypothetically attain larger bandwidth. Scalability of Tesseract also proves not to be an issue.

==+== D. Comments for author

The in-order core chosen for illustration is a ARM Cortex-A5. Has it been tried with other architectures? Does it affect the overall performance?

The evaluation part wraps up with care all the design variables that could be tweaked and shows that Tesseract still outperforms all other existing configurations. However, there is no mention of the cost of adoption for Tesseract and there is still an overhead due to the new API that needs to be accounted for when designing graph-processing algorithms.

As you mentioned, it would be interesting to dive into how to improve graph distribution across vaults but that is for future work.

# Strength

- Notable improvements under any configuration/workload
- scalability of Tesseract
- close to ideal prefetching (1.8% off!)
- full co-design between HW and API
- below the maximum power density

# Weaknesses

- workload imbalance across vaults depending on graph distribution that could lead to 60% of execution time waiting for sync barriers
- no possibility for address translation inside the memory

==+== E. Comments for PC
==-== Hidden from authors.

==+== Scratchpad (for unsaved private notes)

Tesseract (graph processingHW 3d integration to maximize mem BW, efficient communication between mem partitions, 3 a programming interface

baseline improvement of x10 

scalability issue by the memory bandwidth bottleneck (big-data workloads)

access patterns are random

1) development of an efficient mechanism for communication between different Tesseract cores based on message passing.
2) speciliazed HW prefetchers
-> hints given by the programming interface
-> guarantee atomic memroy updates withot requiring software synchronization primitives

GRAPH PROCESING ISSUES

random partenrs due to neighbor traversal, poor memroy locality
increaing the nb of cores is ineffective
HBM helps to some extent, but could be better if doesnt need to use the HBM inter-links

Problematic: how to utilize correctly the large amount of mem bandwidth for efficient graph processing in memroy?

PIm 8TB of aggregate internal BW to the in-memory computation units

TESSERACT ARCHITECTURE

Vault contains 32 ARM Cortex-A5 processors, 16-bank DRAM 
each core can access its local DRAM partition only 
partition a dedicated memory controller
No address translation 

In order cores have o sall on L1 cachemiss which is far from the best way of utilizing th  mem BW

Design of two prefetchers:
- list prefetcher
- message-triggeredprefetcher

Message passig with blocking remote function call
ores swtich states, and returns value queried by the other core

NON BLOCKING MSG
used in conjonction with synchronization barriers

PREFETCHing

List prefetching
- with a constat stride prefetched for frequent sequential accesses

Message-triggered Preetching
for random accesses, a mechanism to use with nb remote function call to indicated prefetching of a certain addr
allows for execution of function ONLY when data is rdy -> no stalls!! and it is EXACT prefetching since we know the ddress needed

TESSERACT API
get, putinterrupt, copy, list begin and end for the list table prefetching, barriers to synchronize Tesseract ores


EVALUATIon
Graph algorithms Page rank, Vertex Cover 

outperforms DDR3 and HMC due to  prefetching mechanisms and the large INTERNAL bw of HMCs as 

LOWER memroy access latency (96%)
one core per vault (32 vaults)

Tesseract + conventional BW with taht of HMC-MC
1.8% off- of their ideal implentation 

scaling of tesseract (512 core systems) is limited bc of off-chip communications overhead due to remote function calls
sub-linear perfomance improvemnt 

employ better data partitioning schemes that minimizcommunicationbetween different vaults

--- workload imbalance across vaults depending on graph distribution   if severe unbalance at the beginning of execution when vertex update happen in a single patition -> 60% of execution time waiting for snyc barriers...
==+== End Review
