==+== Computer Architecture Paper Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" UNLESS DIRECTED!
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://safari.ethz.ch/review/architecture21/offline

==+== =====================================================================
==+== Begin Review
==+== Reviewer: Guillaume Thivolet <gthivolet@student.ethz.ch>

==+== Paper Number

65

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

Ready

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

4

==+== B. Reviewer expertise
==-== Choices:
==-==    1. No familiarity
==-==    2. Some familiarity
==-==    3. Knowledgeable
==-==    4. Expert
==-== Enter the number of your choice:

2

==+== C. Paper summary

Performance bottlenecks of critical sections in multi-threaded application cause a serious issue in CMP architectures. Accelerated Critical Sections (ACS) is a novel method that aims at shifting the execution of the serial critical sections onto a larger core, improving scalability and overall speed of the program execution.

Over any workload (coarse or fine grain locking), the methods has proved being efficient with an average of 34% faster execution time on the different programs used to benchmark ACS.
Simultaneous multi-threading and Selective Acceleration of Critical Sections (SEL) allow the large core to prevent creating more bottlenecks in the critical sections execution requests issued by smaller cores. SEL detects false serialization and decides whether or not to execute a critical section on a large core.

The authors compare their work with the most resembling methods (improving locality of shared data and locks, RPC), but shows that although they are analogous, ACS comes out as the best method to accelerate shared-memory parallel programs.

==+== D. Comments for author

# Strength

- No specific overhead for the programmer -> the compiler does all the job for you (ISA extension)
- ACS becomes more attractive as the area budget in terms of cores increases
- No cache transfer overhead as locks are kept to the large core (CSCALL/CSDONE advantage) (no thread context migration)

# Weaknesses

- Delaying critical interrupts to prevent deadlocks when waiting for CSRET is a major drawback
- Lack of opening on scaling the method to a higher number of large cores

- No availability of the code to reproduce the results?

The paper includes lots of figures to backup their claims with numbers, and it shows that extensive benchmarking has been achieved in order to prove the method. ACS is well described and this in an interesting mechanism with which also has positive repercussions such as eliminating transfer of data across caches. This ultimately lowers the number of cache misses.

The only big concern that is left is: is delaying interrupts the best way to prevent deadlock mechanisms? As this potentially could delay critical inputs and there doesn't seem to be workarounds/ways to ensure that this is not happening with ACS.

A few additional remarks.

- Section 3.9 is short / wouldn't the presence of more large cores be equivalent to the initial problem set? 
- 8.4 even if RPC is intended for network programming, it would have still been interesting to compare it to ACS
- what is the area of a large core compared to small cores?

==+== E. Comments for PC
==-== Hidden from authors.

==+== Scratchpad (for unsaved private notes)

Chip-Multiprocessors that provide multiple procesisng cores on a single chip. 
Applications create threads that operate on the same problem and communicate with different portions of a shared memory
Critical section -> only one thread can execute it a time
accelerating executions of critical sections can improve performance and scalability
accelerated critical sections are executed by a large core and notifies the core that requested execution of the instruction
private data is transfered via the regular cache coherence mechanism
ACS reduces the number of L2 cache misses inside the critical sections by 20%

--- false serialization
-> dynamic mechanism that decides whether or not a critical section should be executed on a small core or a large core

+++ new instructions CSCALL [LOCK_ADDR, TARGET_PC] 
                     CSRET  []

Compiler removes register dependencies and inserts those instructions around the critical section

CSDONE response when large core responsd to small core

Critical section request Buffer pensd CSCALL requests sent by small cores, size is * to # cores ( storage overhead of 300 bytes)

Chip-interconnect modifications to support extensions

Simultameous Multi Threading and Selective Acceleration of Critical Sections (SEL)

Selestimates the occurence of false serialization and decides whether or not to execute a critical section on a large core
if the occurence is high, the serialization is executed on the smaller core
reset the ACS_DISABLE bits every 10 million cycles to adapt to phase changes

--- delaying critical interrupts to prevent deadlocks is a major drawback

Transfer of cache data

+++ no cache transfer overhead as locks are kept to the large core (CSCALL/CSDONE advantage)

+++ ACS becomes more attractive as the area budget in terms of # of cores increases

Coarse grained locking improvements (qsort and tsp havs smaller perfomance boosts)
Fine grained locking -> ACS wastes most of the area budget by dedicating it to a large core because it is unable to use the large core efficeiently

ACS Perfomance increases as the area budget increases
ACS with SEL outperfoms w/ SEL by 15%, reduction in average of 34% over the programs benchmarked

+++ No migration of thread contexts aith ACS


==+== End Review
