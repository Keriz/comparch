==+== Computer Architecture Paper Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" UNLESS DIRECTED!
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://safari.ethz.ch/review/architecture21/offline

==+== =====================================================================
==+== Begin Review
==+== Reviewer: Guillaume Thivolet <gthivolet@student.ethz.ch>

==+== Paper Number

67

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

()

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

5

==+== B. Reviewer expertise
==-== Choices:
==-==    1. No familiarity
==-==    2. Some familiarity
==-==    3. Knowledgeable
==-==    4. Expert
==-== Enter the number of your choice:

3

==+== C. Paper summary

Conventional prefetchers suffer from a few issues: (1) lack of adaptability, (2) no system awareness and (3) only 1 pre-defined parameter is chosen for the prediction. They hold a core role in microarchitectures and are essential in order to have energy-efficient, low-latency and high throughput cores. Those prefetchers are designed to perform the same under any workload and number of cores.

Pythia implements a configurable reinforcement learning agent on hardware, and although not as general-purpose as ML to explore diversity of configurations, it provides an area-effective prefetcher that improves perfomances, accuracy and latency.

It outperforms other state-of-the-art prefetchers (MLOP, BINGO, SPP..) and its gain is linear to number of cores.

==+== D. Comments for author

The Q-value division in vaults is a great way of achieving lower storage space, and reminds the CMAC technique in review [37][Self-Optimizing Memory Controllers: A Reinforcement Learning]. Action pruning is a clever way to prevent exponential explosion of the array size. The pipeline for Q-value updates is a welcomed addition as well.

The main downside of automated design space exporation is that it has been done before configuring Pythia for online use, so it seems to be offline DS exploration for an online use. As you have written the features selected are the best-performing ones but also later it is stated that for Ligra worload, the feature selection and reward should be adapted. As far as I understand Pythia is not capable of doing that online.

An explanation of the XOR's in the program features (3 and 7 table 3) could enlighten the readers.

In the algorithm, the Evaluation Queue is an ingenious implementation of the reward system and the 5 level of rewards (especially the no-prefetch) demonstrate their use later in the paper. Would tuning the reward values together rather than individually lead to different results?

The use of unseen traces is a reliable method of showing Pythia's perfomances.

In the evaluation, the power-overhead is not compared to the power-savings of the Pythia solution compared to others prefetchers.

In the multi-level caches, only Pythia+stride have been tested, but could Pythia+Pythia lead to even better results?

# Strength

- Action pruning to save space
- Pythia evaluation using champsim, and perfomance increase
- ++ Tweakable through registers for feature selection and rewards
- Small area and power overhead
- Clever evaluation queue reward system and pipeline

# Weaknesses

- Multi-level prefetching schemes were not evaluated thoroughly
- Pre-training of Pythia for feature selection sorts of defeats the purpose

==+== E. Comments for PC
==-== Hidden from authors.

==+== Scratchpad (for unsaved private notes)

harwdare pretching techniques
PC, cacheline address, or delta between addresses
neglect mem BW or system unaware prefetch algorithm

reinforcement learning agent pythia

program context is a feature

PC, Address, Page offset of a cacheline

prefetcher accuracy reduces the long memroy access latency of the processor

CURRENT PREFETCHER LIMITATIONS
1)use of 1 program feature for prefetch prediction
2)no ystem awareness
3)lack of adaptabilitiy

PYTHIA
silicon customized vi configuration registers (incr/decr. coverage accuracy, timeliness)
state/agent systm with a reward system for prefetch and non-prefetch

learning rate parameter

RL better than machine learning as model size exceed largest caches in traditional processors
quick predictions

PYTHIA
find the maximal prefetching policy that would maximiye the number of accurante and timely prefetch requests

states are a k-dimensional vector of program feautre.
a progrma feature is at most 1(porgram control flow component AND/OR rogram data flow component)

PCF uses PC of a load instruction, branch PC, and history to remember if info is extracted only from the current demand request or a serie of past req

PDF is cacheline address, page offeset, cahce delta, and history..

pythia has a fixed k dimension (limited HW size budget)
5 level of reward 
accurate and timerly/late, loss of coverage, inaccurate
no-prefetch

Q-Value
Evaluation queue has entries
(1) take action, (2)prefetch address gennerated  3) a filled bit (pref req has been filled into cache)

for ever y new demand pythia check the EQ and attributes rewards

EQ evictions are used to updae Q values

QVStore search
table based hierarchical QVS store
k partitions (vault)
each vault is a constituent feature of the state-vector and records te Q values for the feature action pair Q(\phi_s^i)

a vault is a two dimensionnal table indexed by feature and action values

problem is size of the table is exponential
-> tile coding (CMAC)

multi stage pipeline to retrieve maximum Q value
+++ area andpower overhead of the pipeline

reward assignement and Q valu updates
1) immediate reward assignement during EQ inserstion
2) during EQ residency
3) during EQ eviction

pythia outperforms bingo and SPP because of high prefetc acuracy and high prefetch coverage


++++ action pruning
+++ tweakble pythia through registers for  feature selection

---- would tuing the reward values together lead to better results?

+++ pythia evaluation using champsim

EVALUATIoN
incrases linerarly with number of cores
the less dram bandiwth, the better pythia performs in compairosn to MLOP, BINGO

--- multi-level prefetching schemes were not evaluated (pythia on L1, L2, L23

+++ customizion of feature selection and rewards (ie. focusing on NOT prefetch actions) Ligra workload suite

low area and power overheads (1%
==+== End Review
