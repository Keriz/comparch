==+== Computer Architecture Paper Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" UNLESS DIRECTED!
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://safari.ethz.ch/review/architecture21/offline

==+== =====================================================================
==+== Begin Review
==+== Reviewer: Guillaume Thivolet <gthivolet@student.ethz.ch>

==+== Paper Number

87

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

()

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

4

==+== B. Reviewer expertise
==-== Choices:
==-==    1. No familiarity
==-==    2. Some familiarity
==-==    3. Knowledgeable
==-==    4. Expert
==-== Enter the number of your choice:

2

==+== C. Paper summary

Deep Neural Network have a wide range of applications and there is an ever growing set of fields that are using them. Commodity hardware or specialized hardware are used to run them. Their workloads are highly memory intensive leading to vast energy-consumptions that often could be prevented due to the regenerative property of DNNs (error-resilient).

This paper EDEN presents a novel approach for a reduced energy waste by altering two important DRAM parameters: Voltage and Timings. Violating the normalized increase the bit-error rates in the DRAMs but DNNs can tolerate this noise due to their multi-layered structure. Retraining of the DNNs is performed on the modified DRAMs, so that their model plans ahead and accommodate to errors within a 1% tolerance goal. The DNN layers are mapped to different DRAMs partitions that have varying {voltage, timing} configuration, to set the error rate to an appropriate level for each layer.

EDEN has been tested on CPUs, GPUs, DNN accelerator and improve error resilience by x10, with an average of 25% energy reduction and a speedup of up to 8%.


==+== D. Comments for author

A huge positive aspect of EDEN is that it fits all DRAM models produced by any vendor. The well-thought-out procedure and the evaluation models prove that EDEN fits common DRAMs.

EDEN leverages a mechanism not previously applied to DNNs before, hopefully in the future DRAM vendors will not add a sub-voltage threshold stop mechanism to prevent such bit-errors from happening for security concerns.

Circular retraining is a clever way to let EDEN be permeable to errors and boost the DNN accuracy.

As you said, power is proportional to Vdd^2, so for a voltage reduction of 20%, we should see an energy efficiency improvement of 40%, which is not the case. Its not too far off but probably a section explaining the discrepancies could add some value.

# Strength

- Novel technique to reduce DNNs energy consumption
- Achieves sub <1% precision error
- Accuracy collapse defense
- Extensive characterization
- EDEN offloading, or offline EDEN training

# Weaknesses

- division of mapping onto the DRAM partitions, banks, subarrays: does a given layer have to have the same configuration?
- (does not work on reliable dram)

Additional comments:
- Figure 11. I suggest adding "tolerable" next to BER on the Y-axis
- Figure 13. any explanation on poor SqueezeNet energy reduction? or is it linked to the following section

-(side comment) lots of repetitions in the paper that make the read difficult ("we propose eden.."/in the background/eden explanation..),

==+== E. Comments for PC
==-== Hidden from authors.

==+== Scratchpad (for unsaved private notes)

EDEN
increasing memrozntensitz of most DNN workloads, goalis to use approximate memory ( violating timings and reduced supply voltage), leading to higher bit error rates

but neural netowwrk can tolerate increased bit errors

map the memroy error in in a DRAM partition in realation to user specified DNN accurayc requirements
evaluated on multi-core GPUs, DNN accelerators

EDEN DNN retraining improve error siliency by x10 with a 1% accuracy loss, average 25% energy reduction and a speedup (max of 8%..)

ISSUES
dram energy and latency 


approach of eden is to custimnize off-theshelf DRAM chips to better suit the DNN

EDEN 3 principles:
.retraining of DNN with the error tolerance of altered DRAM
.profiling of mproved DNN at all layers of the DNN
.map different DNN data to different DRAM partitions with different voltage and latency violation configuration

user defined accuarcy
---- repetitions of "we propose eden..."

BACKGROUND
Weight matrix, input feature maps, output feature maps
overparametrization is the source of DNNs accuracy
quantization of floating points weigths and OFM into fixed point can greatly improve energy and perforamce

Power is proportionnal to the square of supply voltage (vdd^2xf)

EDEN
1. boost DNN erreror tolearance
2. DNN error tolerance characteriuation
3. DNN DRAN mapping

1.circular reraining to boost DNN error tolerange, it injects the error into the DNN training procedure and boost the DNN accuracy

2. cahracterize IFMS, OFM and weights to identify the lmits of bi eror tolerance using the DNN validation dataset


1. accuracy collape, to provent this we derive slowly from the initial DRAM parameters to a target value , increase every two epoch seems to work relatively well

---- DOESNT NOT WORK ON RELIABLE DRAM

avoid accuracy collapse by not using values that are implausible

DNN error tolerance characterization
coarse grained characterization
logarithmic scale binary search on the error rates

coarse grained -> error tolerance
fine grained -> map highest tolerable bit error rate (BER) of each weight and map it  to a different DRAM portion

---maping of all a layer weight to the same DRAM partitions

EDEN OFFLAOADING
to train EDEN on a DNN without access to the DAM with an error model

chracterized DDR3 and DDR4 DRAM modules
errormodels
1.uniform random distribution across a DRAM bank of bt errors
2. bit errors follow a vertical distriution across the bitlines of a DRAM bank
3. horizontal distribution cross wordlines
4.uniform distrubtion that is data-dependent

model selection estimates a likelihood estimation procedure to determine the parameters of each error model and the error model that is most likely to produce the error

MEMORY CONTROLLER MODIFICATION
correct implausible values with upper and lower bound thresholds ( 1 cycle latency only)

RAm is split into 2^10 partitions 

EVALUATION
accuracy validation of the error model using differet numeric precisions
complete DRAM profiling can take 4 minutes

the way in wichc each error model captures the distribution of weak cells across data layout affects its imppacton the error curev

difference of accuracy across BER and quantization levels depending on the error model

VOLTAGE REDUCED BY 26% and trcd by 48%

----Figure 11. add "tolerable" next to BER

CPU-GPU

---Figure 13 . anyexplanation on poor SqueezeNet energy reduction?
==+== End Review
