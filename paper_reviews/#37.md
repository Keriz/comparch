==+== Computer Architecture Paper Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" UNLESS DIRECTED!
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://safari.ethz.ch/review/architecture21/offline

==+== =====================================================================
==+== Begin Review
==+== Reviewer: Guillaume Thivolet <gthivolet@student.ethz.ch>

==+== Paper Number

37

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

()

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

5

==+== B. Reviewer expertise
==-== Choices:
==-==    1. No familiarity
==-==    2. Some familiarity
==-==    3. Knowledgeable
==-==    4. Expert
==-== Enter the number of your choice:

3

==+== C. Paper summary

This paper presents a self-optimizing memory controller that takes effect on the scheduling policy. Static policies such as FR-FCFS do not adapt to switching workloads, which is where reinforcement learning (RL) methods come in to perform on-the-fly policy changes.

An RL-based memory controller monitors its environment and performs actions (sends commands) relative to the current states (open rows, queue, time of arrival...). The RL controller should not violate the specification timings. Its goal is to prioritize the long term cumulative rewards by learning an optimal scheduling policy. The RL agent is also be able to allow for a certain degree of freedom by scheduling random non-ideal immediate commands but potentially better rewarding in the future.  

The optimal policy algorithm uses the concept of Q-values that represent state->actions pairs, constituting the scheduling policy. After each command issued, the Q-value is updated (rewarded) in function of the new state, the previous action executed, and the previous Q-value. To summarize, Q-value is the expected outcome of the cumulative discounted reward that is obtained after a specific action is executed in a given state. Discounted means that infinite rewards are not possible and each Q-value ends up converging. It is a guarantee that in the long-term, the controller will end up with the optimal policy.

Techniques such as CMAC save SRAM space to store the controller's Q-values while giving a trade-off between coarse and fine granularity. This is necessary due to the exponential number of required Q-values, growing in function of all possible states and actions.

This is also why a preselected optimal set of  states is used,  with the benefit of not impacting the results when evaluated against more states. Baseline FR-FCFS policy is topped by RL-agent (min. +5% bandwidth) across all workloads, with an almost ideal data-bus usage (80%) which is a reliable indicator for performance.

==+== D. Comments for author

The idea of CMAC is very interesting and brings a lot to the implementation of the RL-agent in the hardware. The paper is rather concise and well-written but I feel that the evaluation has focused too much on a specific workload, which the opposite of the versatility RL is intended for. Figure 12 could help showing that.

A few comments:
- No evaluation on reactivity for workloads change has been performed
- If you mention multiple controller, a conclusion opening would have been appreciated on ideas to coordinate them rather than concluding on their autonomy 
- why does offline RL perform better than FR-FCFS? it seems biased because you supposedly trained it before. 

# Strength

- CMAC Q-values subarray division
- improvements compared to FR-FCFS
- number of selectable Q-value per DRAM cycle does not affect too much the outcome

# Weaknesses

- No evaluation on reactivity for workloads changes (how long does it take to switch to a policy better performing than FRFCFS)
- Fairness has not been evaluated
- e, alpha, gamma parameters were not enough evaluated


==+== E. Comments for PC
==-== Hidden from authors.

==+== Scratchpad (for unsaved private notes)

Self optimizing memory controller (scheluding policy)

reinforcement leearning to overcome dynamic workloads imitations
on-the-flyoptimizations
4-core CMP improvement by 19% in aveage

chip multiprocessors

off-chip bandiwth might soon become a limitation as number of pin is reaching a threshold

ad-hoc memroy controller designs

CONTroLLER
RL-based memory controller considers the long-term perfoamcne of impact each action it can take


2) take actions that provides the highest long-term reward
3)update long term reward values associated with state action pairs based on feddback from teh system

DDR2 dual channel dram

DRAM chips needs to obey DRAM timing to provide correct functionality

controller must prioritize DRAM commands from diferent memory requests to optimize erformance

FR-FCFS does not consider the lon-tern perfomance impat of priritizzing a column command ovre a row one


REINFORCEMENT LEARNIGN

agent sense the current state and executes an action
goal: maximize long term cumulative reward by learnin an optimal policy that maps states to actions

rewards can be deign i several ways depending on optimization goals (maximize data bus utilization)

3 challenges
1)temporal credit assigment (for bad past decisions, and predict future decisions, and anticipation for long-term cumulative payoffs
2) exploration VS exploitation: agent needs time to learn about its environment but also should give the best availalble policy at any time

Markov decision processes

next sate in DRAM scheduling may depend on the command scheduled by the DRAM scheduler and also y the system behavior that one can learn its function

CUMULATIVE REWARDS
convergence knob to prevent infinite rewards

Q-VALUES
q value of a state-action pair under policy \pi represents the expected value of the cumulative discounted future reward that is btained when action a is executed in state s

it descrivbes te long term value of scheluing a command in a gven syste, state

STATES
6 attributes considered 

-> optimize the read/write balance
-> detect low levels of request concurency n the transaction queue
-> facilitate learning
-> amortize write to read delays by satisfying writes in bursts
-> approximate nb of critical requeusts that might block the queue

assume an integrated on-chip memroy controller

ALGORITHM
the goal of the algotirhm is to pick the command wwith the hgjest q value for schedling

Q-value update based on the update rule known as the SARSA update
empirical methods to find alpha, gamme and Q

guaranteed to find the optimal scheluding policy wih probability 1

--- not aggressive enough maybe
e greedy action selection (random command with a small probability

GOAL: reduce number of Q values

quantiye the state space into a small nb of cells with a single Q avlue to reduce storage requirements

CMAC balancing generatlization and resolution
multiple coarse grain Q value tables shifted by a random amount, used to represent a higher number of Q-values with a smaller number of cells

hashing can be used (inputs are states) to reduce storage

ENSURE CONTROLLER PROGRESS
no nops
not allowed to activate unsolicitted rows
able to escape starvation due to lock mechnamisms issuesing multiple time the same mem req.
timeout policy
Rl cnnot dictate refresh interval

HADWARE

each CMAC array is an SRAM array f 256 Q values, 32 CAMC arrays in each..


can compute two Q values per cycle, 10 cycles between eah DRAm cycle and 4 cycles necessary to ill up the pipe, so 12 Q-values can be considered each time

floating point arithmetic to compute Q values

32kb on chi storage for 8192 Q values

EVALUATION
evaluation of the hundreds of attributes to find the 6 best perfoming ones

perfomance improvement of 20% (5% min. for all applications) with baseline Fr-FCFS

%0% improvement of the data bus!! 

dual-channel compatibility


--- perform multi-controller coordination in future work
---why offline RL performs better than FCFS? that doesnt seem normal
--- how to find alpha?

---sensitivity (reactivity) to workload change
--- fairness problems
==+== End Review
