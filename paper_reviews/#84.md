==+== Computer Architecture Paper Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" UNLESS DIRECTED!
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://safari.ethz.ch/review/architecture21/offline

==+== =====================================================================
==+== Begin Review
==+== Reviewer: Guillaume Thivolet <gthivolet@student.ethz.ch>

==+== Paper Number

84

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

()

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

5

==+== B. Reviewer expertise
==-== Choices:
==-==    1. No familiarity
==-==    2. Some familiarity
==-==    3. Knowledgeable
==-==    4. Expert
==-== Enter the number of your choice:

3

==+== C. Paper summary

Non-volatile memories come with a drawback: it needs to ensure crash consistency as in the event of a crash, the memory operations could have not all been serviced (stores or writes). DRAM technologies do not suffer such issues as it when the power is turned off, it retains no data. In the case of NVMs, the system needs to be able to pick up where its last state was. Moreover, this partitioning between DRAM and NVMs imply a huge burden for the programmer which needs to design his program carefully, potentially leading to wrong types or access patterns.

Checkpointing is used to restore the microarchitectural and memory state in case of failure. It is a costly operation which depends on the granularity at which the data was saved as well as where (NVMs or DRAM) it is stored (access latency). ThyNVM reduces the stall time by overlapping the application execution and checkpoint time through the help of an OS-transparent hardware address space on the memory controller, that leverages both DRAM and NVM for efficient checkpointing. It employs two levels of granularity, at the cache block level and at a page granularity, which are stored on NVM and DRAM respectively.

ThyNVM is formally proven and has been evaluated on different access patterns against 4 other systems. It reduces NVM write traffic by 12% in average, while providing a performance increase against competitors due to the overlapping of runtime and consistency check.

==+== D. Comments for author

Section 4 is rather hard to follow and would necessitate additional illustrations to comprehend more easily (example, changing the notations). For instance, in 4.2 I understand that the mechanism to transfer from BTT to PTT happens at the beginning of an epoch. In 4.3 it is not mentioned how an entry is added into the PTT.

The store counter thresholds have been determined empirically, how does their value impact the bandwidth or performance (ie. doubling or halving).  other numbers (doubling or halving).

Overall, the three epochs mechanism is well designed as well as the  invisible partitioning for the software. A noticeable improvement is how few storage space it requires compared to logging-based methods.

Woulnd't it possible that there are multiple entries indexed at the same offset, so that hardware addresses collide (in BTT and PTT)?

You could also provide a graph which plots the NVM memory write bandwidth usage VS the stall time for ThyNVM.

Additional comments:
- In the figure 9, we can hardly distinguish ThyNVM from the others
- Figure 6 was a great addition

# Strength

- Two-granularity levels and clever synchronization system 
- Software transparent
- Scalable and improvements if we increase BTT size

# Weaknesses

- Source code and simulation setup not available anymore
- Loss of external states on system failures (ie. network card) anything that could be done in that regard?


==+== E. Comments for PC
==-== Hidden from authors.

==+== Scratchpad (for unsaved private notes)

Byte addressable non-volaitle memories (NVMs)
promise persistent memroy
need to guaranteee crash consistency
-> partition persistent and transient memroy data
2) use specialized software interface when updating memroy data

ThyNVM is a hardware assistes DRAM+NVM persistnet memory design
ThyNVM supports software transparent crash consistency of memory data in a hybrid memory system
dual scheme checkpointing time with application execution time

cehckpoint of data at multiple granularities in a coordinated manner
ThyNVM devises a software transparent mechanism to ensure crash consistency in persistent memory systems

goal -> reduce checkpointing time
1) overlaps checkpointing time with application exeecution time 
2) dynamically detemines checkpointing granularity of data by making the new observation of tradeoff between application stall time and metadata storage overhead of checpointing

reduces etadata sotrage overhead
and greatly increase effective memroy bandwidth utilization

-> manually partitionning is burdensome for the programmer
->reference between transient and persistent data in a unified memory space require careful management
->applications need to employ transaction to update persistent data


inefficiency of logging and copy on write (every memory update has to be logged.. and log replay increases the recovery time on system failure)

cehckpointing is a periodical update to NVM of memory data

checkpoint granulartiy is determined be granularity atwhich we keep track of the working/checkpoint data

checkpnt latency is affected by the location of he working copy of data (DRAM or NVRAM)

DESIGN
enforces cras consistency over all memory data transparently to application programs

sparse writes are checkpointed at cache block granularity, dense writes are checkpointed at page granularity

ASSUMPTIONS
failure model > periodic checkpoint of memroy data and CPU state 

epoch has an execution phase amd a checkpointing phase
alternating those two provide a significant performance boost

overlapping epochs to prevent stalls
1) isolate data updates of diferent epochs
2) maintain a safe copy of the active and last epoch

new working copy (for cache blocks) is mapped to a different address in NVM only persists the metadataneeded to locate the last checkpoint in the checkointing phase

page writeback is done on DRAM at page granualrity

dirty pages are written back to NVM only during checkpointing
ThyNVM directs each dirty page to a different address during checkpoingting
PTT tracks the address mappings for pages that are in DRAM

SCHEMES COORIDNATIONS
(switching between the two schemes)

IMPLEMENTATION
address space layout and management
physical address space is different from the memory controller's civew of the HW address space 

1) store different versions of data in different memorylocations in the hardware address space
2) expose a single consistent vesio of the data to the processor upong a load/store access or during system recovery

2 types of data:
- not affected by checpointing (not remapped by BTT/PTT)
- affected

software visible ddress space is not the same as controller's address space

BTT and PTT contain translation of the physical address of each memr req, streering of th dataupdates for checkpointing to appropriate HW address, determinationof when to migrate data between the two checpointing schemes

HW ADDrESS CAN COLLIDE (multiple entries at the same offset?)
store counter increment? how do you determine if you are targeting a single cache block or the whole page

SERVICING LOADS AND STORES

store req uses PTT to determine block remapping or page rite back
performs block remapping for all req that mis in the PTT


+++ figure 6 helpful

---source code and simulation setup not available anymore

EVALUATION
journaling
shadow paging (perf decr. due to full pages wrote back from DRAM to NVM)

adaptivity of ThyNVM to different access patterns reduces overall NVM write traffic across various patterns
2) by overapping eecution with checkpoiniting, thynvm reduces execution time but has more NVM write traffic

--Figure 9 cant see thynvm

BTT size increases trasaction troughput
Whole System Persistence model -> cpu and NVM only

--- Loss of external states on sstem failures (network card)

==+== End Review
