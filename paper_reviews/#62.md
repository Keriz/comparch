==+== Computer Architecture Paper Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" UNLESS DIRECTED!
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://safari.ethz.ch/review/architecture21/offline

==+== =====================================================================
==+== Begin Review
==+== Reviewer: Guillaume Thivolet <gthivolet@student.ethz.ch>

==+== Paper Number

62

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

()

==+== A. Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept
==-== Enter the number of your choice:

5

==+== B. Reviewer expertise
==-== Choices:
==-==    1. No familiarity
==-==    2. Some familiarity
==-==    3. Knowledgeable
==-==    4. Expert
==-== Enter the number of your choice:

3

==+== C. Paper summary

Although the cost per bit in the DRAM technology has decreased by 16x, the access latency has remained relatively constant throughout the years (30% reduction). This bottleneck is caused by long bitlines in the subarrays of DRAMS to which cells are attached. Their long length increase the line charge time to reach VDD/2 again for the sense-amplifier after a PRECHARGE command (which isolates the bitlines from the cells after a reading). The combined capacitance of both the cell and bitline parasitic one impact the timing constraints.

The key idea in Tiered-Latency DRAM (TL-DRAM) is to divide the bitline into two sides by adding an isolation transistor which adds a high impedance from the sense-amplifier standpoint and keep the cost-per-bit stable. This reduces the seen capacitance for cells closer to the sense-amplifiers and thus the access latency. The two sides are denoted as the near and far segment. The notable downside is an increased power consumption and access latency for the far-segment.

To circumvent this, the goal is to access more rows in the near segment than in the far one. Efficient cache management could take place by doing an OS-transparent hardware managed cache, or letting the OS access the near-segments. Three cache management algorithms (LRU-cache, Wait-Minimzed Caching and Benefit Based Caching) have been evaluated for the OS-transparent one. The OS-managed method was tested with an exclusive cache and a profile-based base mapping.  An intelligent inter-segment data transfer without additional overhead on the die, reusing the capability of the sense amplifier was used to not go through the DRAM controller to move data between near and far segments, which would drastically increase latency and cut off the segmentation benefits.

The evaluation uses multiple DRAM-benchmarks to find the best-fitting ratio of near-to-far segments for a fixed number of cells per bitline. As an example, it is showed that for a fixed sized of 512 cells, an ideal number of 32 near-segments improves the performances by ~10% compared to the baseline. The OS-managed near-segment thorough testing has been left for future work. The main conclusion is that there's a trade-off between the number of near-segments, the cache algorithm used and ideal latency. However, the overall power is reduced because of the higher number requests serviced by the near-segment.

==+== D. Comments for author

Partitioning the segments into more than two could further improve the performances, at the cost of ever-increasing latency for further segments and algorithm complexity for cache management. It would be interesting to try out a 3-segments DRAM for instance.

Enforcing a near-segment cache policy might not be the best way to deal with fast-switching workloads (ie stream then random).s

# Strength

- Efficient strategy with lots of areas for improvement (cache policy)
- Low area overhead for the isolation transistor
- Inter-segment data transfer in DRAM
- Concise paper, reader is not lost in the explanations

# Weaknesses

- Weak evaluation of Profile-based page mapping
- No appendix on equivalent bitline capacitance computation (w/ or w/o isolation transistor)

Additional comment:

-Figure 14. summarizes strongly performance improvement of TL-DRAM!

==+== E. Comments for PC
==-== Hidden from authors.

==+== Scratchpad (for unsaved private notes)

Tiered latency DRAM

memroz latency of DRAM has remained constant, beocming the bottlneck
key metric is the DRAM latency decreased by 30% while the cost per bit has decreased by x16!

long bitlines are the dominant source of DRAM latency

each bitline is split in two with the use of an isolation transistor

near segment VS far segment

isolation transistor is turned of for the near segment, sense amplifier only sees this side of the load (at the cost of inreased latency in the far segment due to the extra load of the transistor)

near segment is used as a hardware managed cache in the DRAM
MC maps pages into the near segment, and one where OS does profiling to map frenquently accessed pages to the near segment

bitline at 0.5 VDD. when access transistor turned on, either goes to 0.25 or 0.75VDD. then the sense-amplifier amplifies tue perturbation and injects charge into the cell capacitor and the bitline parasitic capacitor. The cell charge is then restored to its initial value

PRECHARGE turn off the access transistor so the cell is decoupled from the bitline and is not affected by future changes in the bitline voltage

sense amplifier injects charge from the bitline parasitic capacitor to reach 0.5VDD again.

BITLINE LENGTHS

aggregate capacitance of bitline and cell determine how fast the sense amplifer charges the biltline to VDD/2

DRAM area size is inversely prop. to numb of cell per bitline

PLOT of segment length for a fixed max nb of cells per bitline

1.83% overhead for a single isolation transistor

INTER SEGMENT DATA TRANSFER 

data must be read out to DRAM controller and written back
TL-DRAM connects the cells in both near and far semgnet before another activate

LEVERAGING TL_DRAN

1.OS-transparent cache

near segment as an MRU cache

WaitMinimizedCaching (two consecutive accesses)
cache the first access in the near-segment to minimize the seond access latency, and then serve the first

benefit based caching
keep track of cycles benefits (in terms of DRAM cycles saved for subsequent accesses and latency reduction)

2. OS-Managed segments 
exclusive cache
-> managed with a simple policy at the cost of overhead rows (dummy, to-be-cached rows), 0.2% of a subarray

profile-based page mapping
-> bits reserved in the physical address for the segment magement
introduction of an additional near segment row decoder to activate two rows at the same time

--- additionnal power due to isolation transistors
--- Underevaluation of Profile-based page mapping
==+== End Review
